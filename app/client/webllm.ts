"use client";

import { createContext } from "react";
import {
  InitProgressReport,
  prebuiltAppConfig,
  ChatCompletionMessageParam,
  ServiceWorkerMLCEngine,
  ChatCompletionChunk,
  ChatCompletion,
  WebWorkerMLCEngine,
} from "@neet-nestor/web-llm";

import { ChatOptions, LLMApi, LLMConfig, RequestMessage } from "./api";

const KEEP_ALIVE_INTERVAL = 5_000;

type ServiceWorkerWebLLMHandler = {
  type: "serviceWorker";
  engine: ServiceWorkerMLCEngine;
};

type WebWorkerWebLLMHandler = {
  type: "webWorker";
  engine: WebWorkerMLCEngine;
};

type WebLLMHandler = ServiceWorkerWebLLMHandler | WebWorkerWebLLMHandler;

export class WebLLMApi implements LLMApi {
  private llmConfig?: LLMConfig;
  private initialized = false;
  webllm: WebLLMHandler;

  constructor() {
    if ("serviceWorker" in navigator && navigator.serviceWorker.controller) {
      console.log("Service Worker API is available and in use.");
      this.webllm = {
        type: "serviceWorker",
        engine: new ServiceWorkerMLCEngine(
          navigator.serviceWorker.controller,
          KEEP_ALIVE_INTERVAL,
        ),
      };
    } else {
      console.log(
        "Service Worker API is unavailable. Falling back to use web worker.",
      );
      this.webllm = {
        type: "webWorker",
        engine: new WebWorkerMLCEngine(
          new Worker(new URL("../worker/web-worker.ts", import.meta.url), {
            type: "module",
          }),
        ),
      };
    }
  }

  async initModel(onUpdate?: (message: string, chunk: string) => void) {
    if (!this.llmConfig) {
      throw Error("llmConfig is undefined");
    }
    this.webllm.engine.setInitProgressCallback((report: InitProgressReport) => {
      onUpdate?.(report.text, report.text);
    });
    if (this.webllm.type === "serviceWorker") {
      await this.webllm.engine.init(this.llmConfig.model, this.llmConfig, {
        ...prebuiltAppConfig,
        useIndexedDBCache: this.llmConfig.cache === "index_db",
      });
    } else {
      await this.webllm.engine.reload(this.llmConfig.model, this.llmConfig, {
        ...prebuiltAppConfig,
        useIndexedDBCache: this.llmConfig.cache === "index_db",
      });
    }
    this.initialized = true;
  }

  isConfigChanged(config: LLMConfig) {
    return (
      this.llmConfig?.model !== config.model ||
      this.llmConfig?.cache !== config.cache ||
      this.llmConfig?.temperature !== config.temperature ||
      this.llmConfig?.top_p !== config.top_p ||
      this.llmConfig?.presence_penalty !== config.presence_penalty ||
      this.llmConfig?.frequency_penalty !== config.frequency_penalty
    );
  }

  async chat(options: ChatOptions): Promise<void> {
    if (!this.initialized || this.isDifferentConfig(options.config)) {
      this.llmConfig = { ...(this.llmConfig || {}), ...options.config };
      try {
        await this.initModel(options.onUpdate);
      } catch (err) {
        console.error("Error while initializing the model", err);
        options?.onError?.(err as Error);
        return;
      }
    }

    let reply: string | null = "";
    try {
      reply = await this.chatCompletion(
        !!options.config.stream,
        options.messages,
        options.onUpdate,
      );
    } catch (err: any) {
      let errorMessage = err.message || err.toString() || "";
      if (!errorMessage.includes("Please call `Engine.reload(model)` first")) {
        console.error("Error in chatCompletion", err);
        if (
          errorMessage.includes("WebGPU") &&
          errorMessage.includes("compatibility chart")
        ) {
          // Add WebGPU compatibility chart link
          errorMessage = errorMessage.replace(
            "compatibility chart",
            "[compatibility chart](https://caniuse.com/webgpu)",
          );
        }
        options.onError?.(errorMessage);
        return;
      }
      // Service worker has been stopped. Restart it
      try {
        await this.initModel(options.onUpdate);
      } catch (err) {
        console.error("Error while initializing the model", err);
        options?.onError?.(err as Error);
        return;
      }
      try {
        reply = await this.chatCompletion(
          !!options.config.stream,
          options.messages,
          options.onUpdate,
        );
      } catch (err: any) {
        console.error("Error in chatCompletion", err);
        options.onError?.(err as Error);
        return;
      }
    }

    if (reply) {
      options.onFinish(reply);
    } else {
      options.onError?.(new Error("Empty response generated by LLM"));
    }
  }

  async abort() {
    await this.webllm.engine?.interruptGenerate();
  }

  async usage() {
    return {
      used: 0,
      total: 0,
    };
  }

  isDifferentConfig(config: LLMConfig): boolean {
    if (!this.llmConfig) {
      return true;
    }

    // Compare required fields
    if (this.llmConfig.model !== config.model) {
      return true;
    }

    // Compare optional fields
    const optionalFields: (keyof LLMConfig)[] = [
      "temperature",
      "top_p",
      "stream",
      "presence_penalty",
      "frequency_penalty",
    ];

    for (const field of optionalFields) {
      if (
        this.llmConfig[field] !== undefined &&
        config[field] !== undefined &&
        config[field] !== config[field]
      ) {
        return true;
      }
    }

    return false;
  }

  async chatCompletion(
    stream: boolean,
    messages: RequestMessage[],
    onUpdate?: (message: string, chunk: string) => void,
  ) {
    let reply: string | null = "";

    const completion = await this.webllm.engine.chatCompletion({
      stream: stream,
      messages: messages as ChatCompletionMessageParam[],
    });

    if (stream) {
      const asyncGenerator = completion as AsyncIterable<ChatCompletionChunk>;
      for await (const chunk of asyncGenerator) {
        if (chunk.choices[0].delta.content) {
          reply += chunk.choices[0].delta.content;
          onUpdate?.(reply, chunk.choices[0].delta.content);
        }
      }
      return reply;
    }
    return (completion as ChatCompletion).choices[0].message.content;
  }
}

export const WebLLMContext = createContext<WebLLMApi | undefined>(undefined);
